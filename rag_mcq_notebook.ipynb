{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pushpendra-saini-pks/MCQ_Generation-using-RAG/blob/main/rag_mcq_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9171f42a",
      "metadata": {
        "id": "9171f42a"
      },
      "source": [
        "\n",
        "# RAG-based MCQ Generation from a Python Language PDF (Notebook)\n",
        "\n",
        "**What this notebook does (overview):**\n",
        "1. Downloads a free Python book PDF (links provided below).  \n",
        "2. Extracts text from the PDF and splits it into searchable chunks.  \n",
        "3. Builds embeddings (instructions + placeholder code using `sentence-transformers`).  \n",
        "4. Creates a RAG retrieval pipeline (using FAISS locally) and a prompt to a text generator to produce MCQs.  \n",
        "5. Includes a small **toy demo** which runs without external APIs to show sample MCQ outputs.\n",
        "\n",
        "**Free PDF recommendations used in this notebook (you can change to any PDF):**\n",
        "- *Think Python* (Allen B. Downey) — official PDF (Creative Commons / free edition). Recommended URL (open): https://greenteapress.com/thinkpython2/thinkpython2.pdf.\n",
        "- *A Byte of Python* (Swaroop C H) — free PDF: https://www.ibiblio.org/swaroopch/byteofpython.pdf.\n",
        "\n",
        "> Notes: This notebook contains both full pipeline code and a minimal toy demo that **runs without internet or API keys** (so you can see example outputs immediately). For full-scale execution (embeddings + LLM), install required libraries and provide API keys as indicated in the cells.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba5d734e",
      "metadata": {
        "id": "ba5d734e"
      },
      "source": [
        "\n",
        "## Installation\n",
        "The full pipeline uses these Python packages. Install them before running the corresponding cells :\n",
        "```bash\n",
        "pip install pysimplegui PyPDF2 pdfplumber sentence-transformers faiss-cpu transformers openai\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install -q pdfplumber sentence-transformers faiss-cpu google-generativeai\n"
      ],
      "metadata": {
        "id": "1jKvEBXxST38"
      },
      "id": "1jKvEBXxST38",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "7a1843f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a1843f2",
        "outputId": "c495a85d-78a3-4eb0-d60a-480c2f8eed15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skip downloading in this demo. Place your PDF as \"python_book.pdf\" or uncomment the download code.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 1) Download a PDF (optional) - uncomment and change the URL if you want to fetch directly from the web.\n",
        "# Example (uncomment to run):\n",
        "# import requests\n",
        "# url = \"https://greenteapress.com/thinkpython2/thinkpython2.pdf\"\n",
        "# r = requests.get(url)\n",
        "# open(\"thinkpython2.pdf\", \"wb\").write(r.content)\n",
        "#\n",
        "# Alternatively, place a PDF named 'python_book.pdf' next to this notebook.\n",
        "print('Skip downloading in this demo. Place your PDF as \"python_book.pdf\" or uncomment the download code.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM Configuration"
      ],
      "metadata": {
        "id": "BHYv91j2lLDA"
      },
      "id": "BHYv91j2lLDA"
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure Google Generative AI (Gemini)\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')   # Load your stored key\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "GEN_MODEL = genai.GenerativeModel(\"gemini-flash-latest\")\n",
        "print(\"Configured Gemini model:\", GEN_MODEL)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SdIKm6ThjV7",
        "outputId": "2564cd82-8afc-4b2e-f5b9-315c1a40df6a"
      },
      "id": "8SdIKm6ThjV7",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configured Gemini model: genai.GenerativeModel(\n",
            "    model_name='models/gemini-flash-latest',\n",
            "    generation_config={},\n",
            "    safety_settings={},\n",
            "    tools=None,\n",
            "    system_instruction=None,\n",
            "    cached_content=None\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document loading"
      ],
      "metadata": {
        "id": "tU5hJRvdlHYe"
      },
      "id": "tU5hJRvdlHYe"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Extract text from thinkpython2.pdf (uploaded)\n",
        "import pdfplumber, os\n",
        "pdf_path = 'thinkpython2.pdf'\n",
        "if not os.path.exists(pdf_path):\n",
        "    raise FileNotFoundError(f\"PDF not found at {pdf_path}. Please upload it to the notebook directory.\")\n",
        "def extract_text_from_pdf(path, max_pages=None):\n",
        "    text_chunks = []\n",
        "    with pdfplumber.open(path) as pdf:\n",
        "        pages = pdf.pages if max_pages is None else pdf.pages[:max_pages]\n",
        "        for p in pages:\n",
        "            page_text = p.extract_text() or ''\n",
        "            text_chunks.append(page_text)\n",
        "    return '\\n\\n'.join(text_chunks)\n",
        "\n",
        "raw_text = extract_text_from_pdf(pdf_path, max_pages=None)\n",
        "print('Extracted characters:', len(raw_text))\n",
        "# show a short preview\n",
        "print(raw_text[:1000].replace('\\n',' '))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWPA_lEYXma1",
        "outputId": "7f2a7d7b-73cd-4169-9e29-0624f2891979"
      },
      "id": "ZWPA_lEYXma1",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted characters: 402460\n",
            "Think Python How to Think Like a Computer Scientist 2ndEdition,Version2.4.0    Think Python How to Think Like a Computer Scientist 2ndEdition,Version2.4.0 Allen Downey Green Tea Press Needham,Massachusetts  Copyright©2015AllenDowney. GreenTeaPress 9WashburnAve NeedhamMA02492 Permission is granted to copy, distribute, and/or modify this document under the terms of the CreativeCommonsAttribution-NonCommercial3.0UnportedLicense,whichisavailableathttp: //creativecommons.org/licenses/by-nc/3.0/. TheoriginalformofthisbookisLATEXsourcecode.CompilingthisLATEXsourcehastheeffectofgen- eratingadevice-independentrepresentationofatextbook,whichcanbeconvertedtootherformats andprinted. TheLATEXsourceforthisbookisavailablefromhttp://www.thinkpython.com  Preface The strange history of this book InJanuary1999IwaspreparingtoteachanintroductoryprogrammingclassinJava. Ihad taughtitthreetimesandIwasgettingfrustrated. Thefailurerateintheclasswastoohigh and,evenforstudentswhosucceeded,theoveralllevelofachieve\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## chunking"
      ],
      "metadata": {
        "id": "qE9L3jsNlCUi"
      },
      "id": "qE9L3jsNlCUi"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "8f2a81aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f2a81aa",
        "outputId": "315460d0-1944-459c-e1a9-06f0721935e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks: 157\n",
            "Sample chunk (first 500 chars):\n",
            " Think Python How to Think Like a Computer Scientist 2ndEdition,Version2.4.0 Think Python How to Think Like a Computer Scientist 2ndEdition,Version2.4.0 Allen Downey Green Tea Press Needham,Massachusetts Copyright©2015AllenDowney. GreenTeaPress 9WashburnAve NeedhamMA02492 Permission is granted to copy, distribute, and/or modify this document under the terms of the CreativeCommonsAttribution-NonCommercial3.0UnportedLicense,whichisavailableathttp: //creativecommons.org/licenses/by-nc/3.0/. Theorigi\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 2) Chunk the text into overlapping windows (simple word-based chunking)\n",
        "def chunk_text(text, chunk_size=300, overlap=50):\n",
        "    tokens = text.split()\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        chunk = ' '.join(tokens[i:i+chunk_size])\n",
        "        chunks.append(chunk)\n",
        "        i += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_text(raw_text, chunk_size=300, overlap=50)\n",
        "print('Number of chunks:', len(chunks))\n",
        "print('Sample chunk (first 500 chars):\\n', chunks[0][:500])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93f99314",
      "metadata": {
        "id": "93f99314"
      },
      "source": [
        "\n",
        "##  Embedding & Indexing (instructions)\n",
        "\n",
        "Below is example code to create embeddings using `sentence-transformers` and index them with FAISS.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "8dc59b40",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "f6258457c48d40a6bc2baed42408032b",
            "d1fe61d9230245b9a9f332c942d8391b",
            "6f303a75096046ef83441af69c7a8616",
            "003c76b54b8b404998032205300329ca",
            "f8f2af0182e248d99b2e4357ee1b1a1a",
            "82659a8cc4f842c283934177a3eaf210",
            "98f0af12cec94bddbbddee19e8c6a410",
            "9c47010a957a405c8b78c10c13ca0c70",
            "c78e4ab9ce7744f1a38fea5d63e9a580",
            "1509872b8cd14a6eb54ed38e8fd2f66b",
            "d1b115365ca8460682894500901c053e"
          ]
        },
        "id": "8dc59b40",
        "outputId": "d5baa7d5-c5b1-465e-a9df-929909f36418"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6258457c48d40a6bc2baed42408032b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings shape: (157, 384)\n",
            "FAISS index built with 157 vectors\n"
          ]
        }
      ],
      "source": [
        "# 3) Create embeddings locally using sentence-transformers and build FAISS index\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "embed_model = SentenceTransformer('all-MiniLM-L6-v2')  # small & fast\n",
        "# encode all chunks\n",
        "embeddings = embed_model.encode(chunks, show_progress_bar=True, convert_to_numpy=True)\n",
        "print('Embeddings shape:', embeddings.shape)\n",
        "\n",
        "# Build FAISS index (L2)\n",
        "dim = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(embeddings.astype('float32'))\n",
        "print('FAISS index built with', index.ntotal, 'vectors')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a094ee5",
      "metadata": {
        "id": "3a094ee5"
      },
      "source": [
        "\n",
        "##  RAG retrieval + MCQ generation (instructions + example prompt)\n",
        "\n",
        "After building an index, the typical flow is:\n",
        "1. For a prompt or chapter, retrieve top-k chunks from the index.  \n",
        "2. Construct a generation prompt that includes those chunks as context and asks the LLM to produce MCQs.  \n",
        "3. Send the prompt to a text-generation model (Gemini,OpenAI, local transformer, etc.).\n",
        "\n",
        "**Example prompt template (to send to an LLM):**\n",
        "\n",
        "```\n",
        "Context: <retrieved text chunks>\n",
        "\n",
        "Task: Generate 4 multiple-choice questions (each with 4 options) based only on the context.\n",
        "Mark the correct answer with (Correct).\n",
        "Keep each question short and focused on facts from the context.\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "90a086fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90a086fd",
        "outputId": "7f0ecbeb-5547-41ab-8ee2-776b850c9484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top chunk(s) for \"What is Python?\":\n",
            "--- chunk id 0 len 4522\n",
            "Think Python How to Think Like a Computer Scientist 2ndEdition,Version2.4.0 Think Python How to Think Like a Computer Scientist 2ndEdition,Version2.4.0 Allen Downey Green Tea Press Needham,Massachusetts Copyright©2015AllenDowney. GreenTeaPress 9WashburnAve NeedhamMA02492 Permission is granted to copy, distribute, and/or modify this document under the terms of the CreativeCommonsAttribution-NonComm\n",
            "\n",
            "--- chunk id 1 len 7413\n",
            "youwantto. • ForChapter4.1Iswitchedfrommyownturtlegraphicspackage,calledSwampy,toa morestandardPythonmodule,turtle,whichiseasiertoinstallandmorepowerful. • I added a new chapter called “The Goodies”, which introduces some additional Pythonfeaturesthatarenotstrictlynecessary,butsometimeshandy. Ihopeyouenjoyworkingwiththisbook,andthatithelpsyoulearntoprogramandthink likeacomputerscientist,atleastali\n",
            "\n",
            "--- chunk id 122 len 3444\n",
            "two birth dates and computes their DoubleDay. 4. For a little more challenge, write the more general version that computes the day when one personisntimesolderthantheother. Solution: https://thinkpython.com/code/double.py Chapter 17 Classes and methods Although we are using some of Python’s object-oriented features, the programs from the last two chapters are not really object-oriented because the\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 4) Retrieval helper: embed a query with the same model and return top-k chunks\n",
        "def retrieve(query, top_k=4):\n",
        "    q_emb = embed_model.encode([query], convert_to_numpy=True)\n",
        "    D, I = index.search(q_emb.astype('float32'), top_k)\n",
        "    results = []\n",
        "    for idx in I[0]:\n",
        "        results.append({'chunk': chunks[idx], 'chunk_id': int(idx)})\n",
        "    return results\n",
        "\n",
        "# quick test\n",
        "print('Top chunk(s) for \"What is Python?\":')\n",
        "for r in retrieve('What is Python?', top_k=3):\n",
        "    print('--- chunk id', r['chunk_id'], 'len', len(r['chunk']))\n",
        "    print(r['chunk'][:400].replace('\\n',' '))\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate MCQ using Gemini"
      ],
      "metadata": {
        "id": "nkznrgrwYp1C"
      },
      "id": "nkznrgrwYp1C"
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Generation: use Gemini to create MCQs from retrieved context\n",
        "def generate_mcqs_with_gemini(context, num_questions=4):\n",
        "    prompt = f\"\"\"You are a helpful MCQ generator.\n",
        "Based ONLY on the context below, generate {num_questions} multiple-choice questions.\n",
        "Each question should have 4 options (A-D). Mark the correct option by appending ' (Correct)' after it.\n",
        "Be factual and keep each question concise.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "    # Use the generative model to create text output\n",
        "    response = GEN_MODEL.generate_content(prompt)\n",
        "    # The response object usually has .text; if not, print the response to inspect available fields.\n",
        "    try:\n",
        "        return response.text\n",
        "    except Exception:\n",
        "        # fallback: return full response repr for debugging\n",
        "        return repr(response)\n",
        "\n",
        "# Example end-to-end: retrieve, build context, generate\n",
        "query = 'Variables in Python and assignment statements'\n",
        "retrieved = retrieve(query, top_k=4)\n",
        "context = '\\n\\n'.join([r['chunk'] for r in retrieved])\n",
        "print('Context length (chars):', len(context))\n",
        "\n",
        "mcq_text = generate_mcqs_with_gemini(context, num_questions=4)\n",
        "print('\\n=== Generated MCQs ===\\n')\n",
        "print(mcq_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "m2f70K79Yi7N",
        "outputId": "4794cd59-d057-425c-9e44-c71a410bf356"
      },
      "id": "m2f70K79Yi7N",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context length (chars): 13106\n",
            "\n",
            "=== Generated MCQs ===\n",
            "\n",
            "**Question 1:**\n",
            "In Python, why is the assignment statement (`a = b`) fundamentally different from a mathematical proposition of equality?\n",
            "A. Mathematical equality must be explicitly initialized.\n",
            "B. A proposition of equality is always symmetric, but assignment is not. (Correct)\n",
            "C. Assignment means the claim that `a` and `b` are temporarily equal.\n",
            "D. Mathematical equality changes over time, while assignment is static.\n",
            "\n",
            "**Question 2:**\n",
            "What is the conventional indentation used for the body of a function definition in Python?\n",
            "A. Two spaces\n",
            "B. Three spaces\n",
            "C. Four spaces (Correct)\n",
            "D. A single tab\n",
            "\n",
            "**Question 3:**\n",
            "If an expression other than a variable name is placed on the left side of the assignment operator (e.g., `hours * 60 = minutes`), what type of error results?\n",
            "A. `ValueError`\n",
            "B. `NameError`\n",
            "C. `SyntaxError` (Correct)\n",
            "D. `TypeError`\n",
            "\n",
            "**Question 4:**\n",
            "If a programmer wishes to reassign a global variable inside a function, what specific statement must be used within that function?\n",
            "A. `local`\n",
            "B. `reassign`\n",
            "C. `function`\n",
            "D. `global` (Correct)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6437c0b4",
      "metadata": {
        "id": "6437c0b4"
      },
      "source": [
        "\n",
        "## 7) Scaling up\n",
        "\n",
        "\n",
        "- Use a small, fast embedding model (e.g., `all-MiniLM-L6-v2`) for retrieval; keep chunk size ≈ 200-500 tokens.  \n",
        "- Use FAISS for local dense retrieval or Pinecone/Weaviate for managed vector DBs.  \n",
        "- For generation, you can use OpenAI, Anthropic, or a local Llama/LLM. Restrict generation length and use temperature 0.2–0.7.  \n",
        "- Always include clear instructions in the prompt to keep MCQs factual and avoid hallucinations (include source chunks and ask model to only use them).  \n",
        "- Validate generated MCQs by running an automated heuristic or a small human-review pass.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OksoOPeDknIo"
      },
      "id": "OksoOPeDknIo",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f6258457c48d40a6bc2baed42408032b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d1fe61d9230245b9a9f332c942d8391b",
              "IPY_MODEL_6f303a75096046ef83441af69c7a8616",
              "IPY_MODEL_003c76b54b8b404998032205300329ca"
            ],
            "layout": "IPY_MODEL_f8f2af0182e248d99b2e4357ee1b1a1a"
          }
        },
        "d1fe61d9230245b9a9f332c942d8391b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82659a8cc4f842c283934177a3eaf210",
            "placeholder": "​",
            "style": "IPY_MODEL_98f0af12cec94bddbbddee19e8c6a410",
            "value": "Batches: 100%"
          }
        },
        "6f303a75096046ef83441af69c7a8616": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c47010a957a405c8b78c10c13ca0c70",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c78e4ab9ce7744f1a38fea5d63e9a580",
            "value": 5
          }
        },
        "003c76b54b8b404998032205300329ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1509872b8cd14a6eb54ed38e8fd2f66b",
            "placeholder": "​",
            "style": "IPY_MODEL_d1b115365ca8460682894500901c053e",
            "value": " 5/5 [00:01&lt;00:00,  3.88it/s]"
          }
        },
        "f8f2af0182e248d99b2e4357ee1b1a1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82659a8cc4f842c283934177a3eaf210": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98f0af12cec94bddbbddee19e8c6a410": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c47010a957a405c8b78c10c13ca0c70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c78e4ab9ce7744f1a38fea5d63e9a580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1509872b8cd14a6eb54ed38e8fd2f66b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1b115365ca8460682894500901c053e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}